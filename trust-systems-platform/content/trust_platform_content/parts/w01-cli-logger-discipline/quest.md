---
id: w01-cli-logger-discipline-quest
part: w01-cli-logger-discipline
title: "BOSS FIGHT: Package for Reuse  4h"
order: 6
duration_minutes: 240
prereqs: ["w01-cli-logger-discipline-d05-quest-test-plan-design-2h"]
proof:
  type: "paste_or_upload"
  status: "manual_or_regex"
---

# BOSS FIGHT: Package for Reuse  4h

## Goal

Package your CLI logger into a **clean, reusable project** with a README, a working demo, a test run log, and a baseline performance report. Someone who has never seen your code should be able to clone, build, run, and verify it in under 5 minutes.

By end of this session you will have:

- âœ… A **project README** with structure, build steps, and usage examples
- âœ… A **working demo** that runs all 3 commands (append, read, search) end-to-end
- âœ… A **test run log** proving all 21 tests pass
- âœ… A **baseline report** with timing data and at least one forced failure captured
- âœ… A **clean folder layout** that could be shared as a library

**PASS CRITERIA:**

| # | Criterion | Check |
|---|-----------|-------|
| 1 | README has build + run steps that work | Follow the steps â€” does it build? |
| 2 | Demo runs all 3 commands without error | Run it and verify exit codes |
| 3 | Test log shows â‰¥ 18/21 tests passing | Count PASS lines |
| 4 | Baseline report has timing for append + read + search | Find 3 timing numbers |
| 5 | At least 1 forced failure captured with stderr + exit code | Find the error example |

## What You're Building Today

A **shippable project** â€” the kind you would put on GitHub for another developer to clone, build, run, and verify in under 5 minutes. This is the "final exam" for Week 1.

By end of this session, you will have:
- âœ… File: `week-1/README.md` (project overview with build + run steps)
- âœ… File: `week-1/day6-baseline-report.md` (test results + timing + failure log)
- âœ… A working demo: all 3 commands (append, read, search) run end-to-end
- âœ… A test run log: proof that â‰¥ 18/21 tests pass
- âœ… A clean folder layout that could be shared as a library

What "done" looks like:
```bash
$ cd week-1 && mkdir build && cd build && cmake .. && make
$ ./logger append "System started"     # exit 0
$ ./logger read                         # prints 1 line
$ ./logger search "System"              # prints match
$ bash ../tests/test-cli.sh
# Results: 21 passed, 0 failed
```

You **can**: Share this project and have someone else run it cold.
You **cannot yet**: Handle network I/O â€” that is Week 2.

## Why This Matters

ðŸ”´ **Without packaging, you will:**
- Have scattered files that only work on your machine
- Forget how to build the project in 2 weeks ("what was that cmake command?")
- Be unable to show your work in a job interview or portfolio
- Start Week 2 without a clean baseline to compare against

ðŸŸ¢ **With a shippable package, you will:**
- Have proof of competence: "Here is a working project with passing tests"
- Return to this code in any future week and rebuild it in 60 seconds
- Compare Week 2 performance against Week 1 baseline numbers
- Practice the professional workflow: spec â†’ code â†’ test â†’ package â†’ ship

ðŸ”— **How this connects:**
- **To Days 1-5:** Everything you built this week goes into this package
- **To Week 2:** You start fresh but follow the same packaging pattern for the TCP server
- **To Week 6:** Your backpressure handler will be packaged the same way
- **To Week 12:** Leader election is packaged as a standalone module â€” same structure, bigger system
- **To Week 23:** Your final portfolio demo references these weekly packages as proof of progression

ðŸ§  **Mental model you are building: "Ship Complete Work"**

Incomplete work teaches nothing. Shipped work creates evidence.
A README, passing tests, and a baseline report transform "I worked on something"
into "I built something that works, and here is the proof."

By Week 23, you will have 24 shippable packages â€” each one a portfolio artifact
that demonstrates growing engineering skill. This first package sets the standard.

## Visual Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                DELIVERABLE STRUCTURE                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  week-1/                                             â”‚
â”‚  â”œâ”€â”€ README.md              â† project overview       â”‚
â”‚  â”‚   â”œâ”€â”€ Project structure                           â”‚
â”‚  â”‚   â”œâ”€â”€ Build steps                                 â”‚
â”‚  â”‚   â”œâ”€â”€ Usage examples (3 commands)                 â”‚
â”‚  â”‚   â””â”€â”€ How to run tests                            â”‚
â”‚  â”œâ”€â”€ day1-cli-contract.md   â† from Day 1             â”‚
â”‚  â”œâ”€â”€ day2-logger-write-path.md  â† from Day 2         â”‚
â”‚  â”œâ”€â”€ day3-validation-boundaries.md â† from Day 3      â”‚
â”‚  â”œâ”€â”€ day4-error-catalog.md  â† from Day 4             â”‚
â”‚  â”œâ”€â”€ day5-test-plan.md      â† from Day 5             â”‚
â”‚  â”œâ”€â”€ day6-baseline-report.md â† TODAY                 â”‚
â”‚  â”œâ”€â”€ src/                                            â”‚
â”‚  â”‚   â”œâ”€â”€ main.cpp           â† CLI entry point        â”‚
â”‚  â”‚   â”œâ”€â”€ logger.h           â† logger interface       â”‚
â”‚  â”‚   â””â”€â”€ logger.cpp         â† logger implementation  â”‚
â”‚  â”œâ”€â”€ tests/                                          â”‚
â”‚  â”‚   â”œâ”€â”€ test-cli.sh        â† CLI integration tests  â”‚
â”‚  â”‚   â””â”€â”€ test-run.log       â† captured test output   â”‚
â”‚  â””â”€â”€ CMakeLists.txt         â† build configuration    â”‚
â”‚                                                      â”‚
â”‚  DEMO FLOW:                                          â”‚
â”‚  $ cd week-1 && mkdir build && cd build              â”‚
â”‚  $ cmake .. && make                                  â”‚
â”‚  $ ./logger append "System started"    â†’ exit 0      â”‚
â”‚  $ ./logger append "User connected"    â†’ exit 0      â”‚
â”‚  $ ./logger read 2                     â†’ 2 lines     â”‚
â”‚  $ ./logger search "User"              â†’ 1 match     â”‚
â”‚  $ ./logger search "MISSING"           â†’ exit 1      â”‚
â”‚  $ ./logger append ""                  â†’ exit 2      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Build

Files: `week-1/README.md`, `week-1/day6-baseline-report.md`

## Do

1. **Create the folder layout and README** â€” write a README with exact build steps:
   > ðŸ’¡ *WHY: A README is the first thing anyone reads. If they can't build your project in 5 minutes from your README, the project doesn't exist to them.*
   ```markdown
   # File Logger â€” Week 1

   ## Project Structure
   ```
   week-1/
   â”œâ”€â”€ README.md
   â”œâ”€â”€ CMakeLists.txt
   â”œâ”€â”€ src/
   â”‚   â”œâ”€â”€ main.cpp
   â”‚   â”œâ”€â”€ logger.h
   â”‚   â””â”€â”€ logger.cpp
   â”œâ”€â”€ tests/
   â”‚   â”œâ”€â”€ test-cli.sh
   â”‚   â””â”€â”€ test-run.log
   â””â”€â”€ docs/
       â”œâ”€â”€ day1-cli-contract.md
       â”œâ”€â”€ day2-logger-write-path.md
       â”œâ”€â”€ day3-validation-boundaries.md
       â”œâ”€â”€ day4-error-catalog.md
       â””â”€â”€ day5-test-plan.md
   ```

   ## Build
   ```bash
   cd week-1
   mkdir -p build && cd build
   cmake ..
   make
   ```

   ## Usage
   ```bash
   # Append a log entry
   ./logger append "Server started on port 8080"

   # Read last 5 entries
   ./logger read 5

   # Search for entries
   ./logger search "error"
   ```

   ## Run Tests
   ```bash
   cd tests
   bash test-cli.sh
   ```
   ```

2. **Run a live demo** and capture the output â€” execute all 3 commands and save the terminal session:
   ```bash
   # demo-run.sh
   echo "=== DEMO: append ==="
   ./logger append "System initialized at $(date)"
   echo "exit code: $?"

   echo "=== DEMO: read ==="
   ./logger read
   echo "exit code: $?"

   echo "=== DEMO: search ==="
   ./logger search "System"
   echo "exit code: $?"

   echo "=== DEMO: search no-results ==="
   ./logger search "ZZZZZ"
   echo "exit code: $?"
   ```

3. **Run all tests and capture the log** â€” execute the test script and save to `tests/test-run.log`:
   > ðŸ’¡ *WHY: Captured test output is evidence. "I ran the tests" is a claim. "Here's the log showing 21 passed" is proof. In Week 23 your portfolio references these logs.*
   ```bash
   bash tests/test-cli.sh 2>&1 | tee tests/test-run.log
   # Expected output:
   # T1 PASS
   # T2 PASS
   # ...
   # Results: 21 passed, 0 failed
   ```

4. **Force one failure and capture it** â€” intentionally trigger an error and document exactly what happened:
   ```bash
   # Forced failure: append with empty message
   $ ./logger append ""
   Error: message cannot be empty    â† this goes to stderr
   $ echo $?
   2                                 â† exit code 2 as expected
   ```

5. **Write the baseline report** â€” create `week-1/day6-baseline-report.md` with timing and metrics:
   > ðŸ’¡ *WHY: Baseline numbers let you measure progress. In Week 10, you'll compare WAL write latency against this Week 1 append time. Without a baseline, you can't know if you got faster.*
   ```markdown
   # Baseline Report â€” Week 1

   ## Test Results
   - Tests run: 21
   - Passed: ___
   - Failed: ___
   - Pass rate: ___%

   ## Timing Baseline
   | Operation | Time | Notes |
   |-----------|------|-------|
   | append 1 entry | ___ms | Single write |
   | read 100 entries | ___ms | Full file read |
   | search 100 entries | ___ms | Linear scan |

   ## Forced Failure Log
   Command: `./logger append ""`
   stderr: "Error: message cannot be empty"
   exit code: 2
   Matches error catalog entry: ERR_EMPTY_MSG âœ“

   ## Confidence Check
   - [ ] I can explain the write path from memory
   - [ ] I can list all exit codes without looking
   - [ ] I can name 3 error types from the catalog
   - [ ] My test plan covers happy + error + edge cases
   ```

## Done when

- [ ] README has folder structure, build steps, and usage examples â€” *someone else can build your project cold*
- [ ] Demo runs successfully (all 3 commands work) â€” *proof the implementation matches the spec*
- [ ] Test run log shows â‰¥ 18/21 passing â€” *evidence, not a claim*
- [ ] At least 1 forced failure captured with stderr + exit code â€” *proves error handling works too*
- [ ] Baseline report has timing numbers for 3 operations â€” *Week 2 compares against these*
- [ ] Confidence checklist completed honestly â€” *self-awareness is engineering skill #1*

## Proof

Paste your README (structure + usage section) and the test results summary, or upload both `week-1/README.md` and `week-1/day6-baseline-report.md`.

**Quick self-test:**
> ðŸ’¡ *WHY these questions: Question 1 is the shipping test. Question 3 is the learning test. If you can't answer both, the week isn't done.*

1. Can someone clone your repo and build in under 5 minutes? â†’ If not, your README needs more detail
2. How many tests should pass? â†’ **At least 18 out of 21** (85%+)
3. What was your top failure root cause this week? â†’ Write it in one sentence
