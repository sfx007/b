# ğŸ—ï¸ Distributed Trust Engineer â€” 24-Week Quest Line

> **You are building a civic-grade trust system from scratch in C++.**
> Every week is a new chapter. Every day is a quest. Ship proof or don't move on.

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ®  YOUR CHARACTER CLASS: Distributed Trust Engineer       â•‘
â•‘  ğŸ› ï¸  Weapon of Choice:     C++17/20                         â•‘
â•‘  ğŸ§  Home Base:             Linux                           â•‘
â•‘  â±ï¸  Daily Training:        2h (Monâ€“Fri) Â· 4h (Sat)         â•‘
â•‘  ğŸ–ï¸  Rest Day:              Sunday (light review only)       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ—ºï¸ World Map â€” Quick Jump

| Arc | Months | Theme | Status |
|-----|--------|-------|--------|
| [ğŸŒ Arc 1 â€” Foundations](#-arc-1--foundations-month-1) | Month 1 | Networking & Event Loops | â¬œ |
| [âš”ï¸ Arc 2 â€” Hardening](#-arc-2--hardening-month-2) | Month 2 | Concurrency & Crypto | â¬œ |
| [ğŸ° Arc 3 â€” Distributed Core](#-arc-3--distributed-core-month-3) | Month 3 | Durability & Replication | â¬œ |
| [ğŸ”® Arc 4 â€” Trust Architecture](#-arc-4--trust-architecture-month-4) | Month 4 | CAS, Merkle, Transparency | â¬œ |
| [ğŸ‘‘ Arc 5 â€” CivicTrust Capstone](#-arc-5--civictrust-capstone-month-5) | Month 5 | Full System Composition | â¬œ |
| [ğŸš€ Arc 6 â€” Ship It](#-arc-6--ship-it-month-6) | Month 6 | Portfolio, Demos, Interviews | â¬œ |
| [ğŸ† Weekly Boss Fights](#-weekly-boss-fights-checkpoints) | All | End-of-Week Gates | â¬œ |
| [ğŸ“Š Monthly Level-Ups](#-monthly-level-ups) | All | Milestone Reviews | â¬œ |

---

## ğŸ¯ How to Play

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  ğŸ“– LEARN â”‚â”€â”€â”€â–¶â”‚  ğŸ”¨ DO   â”‚â”€â”€â”€â–¶â”‚  âœ… PROVEâ”‚â”€â”€â”€â–¶â”‚  ğŸ“¦ SHIP â”‚
  â”‚  30 min   â”‚    â”‚  80 min  â”‚    â”‚  20 min  â”‚    â”‚ artifact â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**5 Rules:**
1. ğŸ“– **Learn** â†’ ğŸ”¨ **Do** â†’ âœ… **Prove** â†’ ğŸ“¦ **Ship** â€” every single day
2. Never skip the **ğŸ’¡ Why** and **ğŸ§  Self-Check** â€” they are your XP multiplier
3. Saturday = ğŸ—¡ï¸ **Boss Fight** (integration day). Sunday = ğŸ–ï¸ rest + light review
4. âŒ Do NOT advance to next week unless you pass the **ğŸš© Weekly Gate**
5. ğŸ† Publish the monthly artifact â€” it's your portfolio proof

---

## ğŸŒ³ Skill Tree â€” What Unlocks What

```
 [1] Build/Test Hygiene
      â”‚
      â–¼
 [2] TCP Byte-Stream Thinking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                                              â”‚
      â–¼                                              â”‚
 [3] Event-Loop Multi-Client Servers                 â”‚
      â”‚                                              â”‚
      â–¼                                              â”‚
 [4] Concurrency + Backpressure                      â”‚
      â”‚                                              â–¼
      â–¼                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 [5] Hashing / Signing / Replay Defense â—€â”€â”€â”‚ Reuse compounds â”‚
      â”‚                                    â”‚ at every stage!  â”‚
      â–¼                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 [6] Durable KV (WAL + Recovery)
      â”‚
      â–¼
 [7] Replication + Election + Idempotent Clients
      â”‚
      â–¼
 [8] Content-Addressed Storage + Merkle Proofs
      â”‚
      â–¼
 [9] Transparency Log + Consistency Proofs + Signed Checkpoints
      â”‚
      â–¼
[10] ğŸ‘‘ CivicTrust Capstone + Reliability/Security Narrative
```

### ğŸ”— Contracts You Keep Stable (Your "API")

| Contract | Key Fields |
|----------|-----------|
| **CLI** | Predictable command format, exit codes, stderr errors |
| **Protocol Envelope** | `version`, `msg_type`, `request_id`, `timestamp`, `nonce`, `payload_hash`, `signature`, `key_id` |
| **WAL Record** | `lsn`, `term`, `op`, `key`, `value_hash`, `checksum` |
| **Proof Bundle** | `document_hash`, `leaf_index`, `tree_size`, `inclusion_proof`, `checkpoint_signature`, optional `consistency_proof` |
| **Observability** | Structured logs: `node_id`, `request_id`, `latency_ms`, `result`, `error_code` |

---

# ğŸŒ Arc 1 â€” Foundations (Month 1)

> **ğŸ¯ Mission:** Build networking foundations and event-driven server habits.
>
> **ğŸ§  Mindset unlock:** *"TCP is a stream, not messages. Responsiveness is state management."*

```
Month 1 Progress
[â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0% â€” Week 1 Â· 2 Â· 3 Â· 4
```

---

## ğŸ“— Chapter 1 â€” CLI & Logger Discipline (Week 1)

> **ğŸ¯ Theme:** Build repeatable tools and evidence capture before networking complexity.
>
> **ğŸ†• New skill:** Command contracts + deterministic behavior
> **ğŸ”„ Reinforcement:** C++ basics with stronger constraints

<details>
<summary>ğŸ—“ï¸ <b>Day 1 (Mon) â€” Quest: Define Your CLI Contract</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**CLI contract design and deterministic outputs**

Key takeaways:
1. Input/output is an API
2. Exit codes are behavior
3. Stderr for errors, stdout for data

### ğŸ”¨ Do (80 min)
Define `log append/read/search` behavior and argument rules.

> ğŸ†• **New constraint:** Strict exit-code table for all failure modes.

### âœ… Prove (20 min)
Build a **12-case argument matrix** (valid, missing args, malformed flags).

### ğŸ“¦ Ship
`week-1/day1-cli-contract.md`

### ğŸ’¡ Why This Matters
This day creates your behavior spec *before* implementation. It prevents hidden ambiguity later when tests fail. It unlocks automated CLI regression checks.

### ğŸ§  Self-Check
- [ ] What is a command contract?
- [ ] Why are exit codes part of an API?
- [ ] Which errors must go to stderr?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 2 (Tue) â€” Quest: Logger Write Path</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**File I/O reliability basics**

Key takeaways:
1. Append semantics
2. `fsync` tradeoff
3. Permission failures are common

### ğŸ”¨ Do (80 min)
Plan logger write path and file naming scheme.

> ğŸ†• **New constraint:** Atomic append requirement for each log entry.

### âœ… Prove (20 min)
Simulate permission-denied and missing-directory cases in test notes.

### ğŸ“¦ Ship
`week-1/day2-logger-write-path.md`

### ğŸ’¡ Why This Matters
You turn vague "write logs" into a reliability contract. This sets up evidence capture for all later servers. It unlocks reproducible debugging.

### ğŸ§  Self-Check
- [ ] Why atomic append?
- [ ] What failures must logger handle first-class?
- [ ] When would `fsync` be required?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 3 (Wed) â€” Quest: Validation Boundaries</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Input validation strategy**

Key takeaways:
1. Reject early
2. Normalize paths
3. Cap line length

### ğŸ”¨ Do (80 min)
Define validation rules for message size, file path, and command shape.

> ğŸ†• **New constraint:** Max log record size to prevent memory abuse.

### âœ… Prove (20 min)
Create boundary test list (0 bytes, max bytes, max+1).

### ğŸ“¦ Ship
`week-1/day3-validation-boundaries.md`

### ğŸ’¡ Why This Matters
Systems break at boundaries, not happy paths. This day installs safety limits before networking introduces untrusted input. It unlocks safer protocol handling later.

### ğŸ§  Self-Check
- [ ] What boundary values matter?
- [ ] Why set max record size now?
- [ ] What should happen on max+1?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 4 (Thu) â€” Quest: Error Catalog</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Structured errors and observability**

Key takeaways:
1. Error code taxonomy
2. Machine-parsable logs
3. Request correlation IDs

### ğŸ”¨ Do (80 min)
Define error catalog for CLI/logger operations.

> ğŸ†• **New constraint:** Every failure path maps to one stable error code.

### âœ… Prove (20 min)
Produce an error-to-scenario table with expected user-facing text.

### ğŸ“¦ Ship
`week-1/day4-error-catalog.md`

### ğŸ’¡ Why This Matters
This gives your system a stable language for failure. Later distributed debugging depends on predictable error semantics. It unlocks cleaner monitoring and incident triage.

### ğŸ§  Self-Check
- [ ] Why stable error codes?
- [ ] What is correlation context?
- [ ] How is human text different from machine code?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 5 (Fri) â€” Quest: Test Plan Design</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Test harness planning**

Key takeaways:
1. Golden-file tests
2. Negative tests
3. Deterministic timestamps via injection plan

### ğŸ”¨ Do (80 min)
Design CLI/logger test plan.

> ğŸ†• **New constraint:** Deterministic output even when time is involved.

### âœ… Prove (20 min)
Define pass/fail criteria for 15 tests including malformed inputs.

### ğŸ“¦ Ship
`week-1/day5-test-plan.md`

### ğŸ’¡ Why This Matters
You now have explicit evidence criteria, not "it seems fine." This converts learning into measurable progress. It unlocks confidence for network-layer integration next week.

### ğŸ§  Self-Check
- [ ] What makes a test deterministic?
- [ ] What is a golden file?
- [ ] Why include negative tests first?

</details>

<details>
<summary>ğŸ—¡ï¸ <b>Day 6 (Sat) â€” BOSS FIGHT: Package for Reuse</b> â±ï¸ 4h</summary>

### ğŸ“– Learn (40 min)
**Packaging for reuse**

Key takeaways:
1. Module boundaries
2. Reusable utility library
3. Documentation-as-interface

### ğŸ”¨ Do (180 min)
Consolidate week artifacts into one reusable CLI/logger package.

> ğŸ†• **New constraint:** Module split so networking project can import logger without rewrite.

### âœ… Prove (40 min)
Run full week test matrix and collect baseline execution times.

### ğŸ“¦ Ship
`week-1/README.md` + `week-1/day6-baseline-report.md`

### ğŸ’¡ Why This Matters
This turns week work into a component, not throwaway practice. Reuse starts here and continues all 6 months. It unlocks instrumentation in your TCP servers.

### ğŸ§  Self-Check
- [ ] What is reused next week?
- [ ] Why avoid copy-paste modules?
- [ ] What baseline numbers did you capture?

</details>

---

## ğŸ“— Chapter 2 â€” TCP Echo Server with Stream-Safe Framing (Week 2)

> **ğŸ¯ Theme:** Shift from local file correctness to network correctness.
>
> **ğŸ†• New skill:** Socket lifecycle + partial I/O
> **ğŸ”„ Reinforcement:** Validation/error discipline from Week 1

<details>
<summary>ğŸ—“ï¸ <b>Day 1 (Mon) â€” Quest: TCP Lifecycle Spec</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**TCP lifecycle and stream semantics**

Key takeaways:
1. `connect`/`listen`/`accept` split
2. Stream â‰  message
3. Close handling

### ğŸ”¨ Do (80 min)
Specify single-client echo protocol behavior.

> ğŸ†• **New constraint:** Handle port-in-use startup failure explicitly.

### âœ… Prove (20 min)
Startup/shutdown scenario table including bind failures.

### ğŸ“¦ Ship
`week-2/day1-tcp-lifecycle-spec.md`

### ğŸ’¡ Why This Matters
This day defines server behavior before coding details spread. It anchors all future protocol constraints. It unlocks deterministic network tests.

### ğŸ§  Self-Check
- [ ] Why is TCP a byte stream?
- [ ] What happens when port is busy?
- [ ] What is accept socket vs listen socket?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 2 (Tue) â€” Quest: Partial I/O Mastery</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Read/write loop correctness**

Key takeaways:
1. Partial reads happen
2. Partial writes happen
3. Loops must continue until done

### ğŸ”¨ Do (80 min)
Plan server and client loops for full-buffer send/recv behavior.

> ğŸ†• **New constraint:** Never assume one `recv` equals one message.

### âœ… Prove (20 min)
Define test where payload is intentionally fragmented.

### ğŸ“¦ Ship
`week-2/day2-partial-io-plan.md`

### ğŸ’¡ Why This Matters
This is the first major systems reality check. Correct stream handling prevents subtle data corruption later. It unlocks robust framing and replay-safe protocols.

### ğŸ§  Self-Check
- [ ] What is partial read?
- [ ] What is partial write?
- [ ] Why is one `recv` unsafe for message parsing?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 3 (Wed) â€” Quest: Frame Parser</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Protocol framing basics**

Key takeaways:
1. Length-prefix framing
2. Frame size limits
3. Malformed frame rejection

### ğŸ”¨ Do (80 min)
Define frame format and parser states.

> ğŸ†• **New constraint:** Reject oversize frame before allocation.

### âœ… Prove (20 min)
Build parser test table: short header, truncated payload, oversize length.

### ğŸ“¦ Ship
`week-2/day3-frame-parser-spec.md`

### ğŸ’¡ Why This Matters
Framing turns raw bytes into safe messages. It is required for signatures and replay defense later. It unlocks multi-client event-loop reliability.

### ğŸ§  Self-Check
- [ ] Why length-prefix over delimiter here?
- [ ] What is a parser state machine?
- [ ] When should server close the connection?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 4 (Thu) â€” Quest: Timeout Policy</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Timeouts and dead peers**

Key takeaways:
1. Read timeout
2. Heartbeat (optional)
3. Idle connection cleanup

### ğŸ”¨ Do (80 min)
Define idle and read timeout policy for client/server.

> ğŸ†• **New constraint:** Connection closes after idle threshold with explicit reason.

### âœ… Prove (20 min)
Design slow-client timeout scenario and expected log output.

### ğŸ“¦ Ship
`week-2/day4-timeout-policy.md`

### ğŸ’¡ Why This Matters
Timeouts protect resource usage and keep services responsive. This prevents dead connections from draining capacity. It unlocks backpressure policy in Month 2.

### ğŸ§  Self-Check
- [ ] Why are timeouts mandatory in servers?
- [ ] What is idle vs read timeout?
- [ ] How should timeout appear in logs?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 5 (Fri) â€” Quest: Client Retry Rules</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Client observability and retries**

Key takeaways:
1. Retry budget
2. Backoff basics
3. Distinguish transport vs protocol errors

### ğŸ”¨ Do (80 min)
Define client retry behavior for connection failures.

> ğŸ†• **New constraint:** Bounded retries to prevent retry storms.

### âœ… Prove (20 min)
Simulate server-down case and record retry timeline.

### ğŸ“¦ Ship
`week-2/day5-client-retry-rules.md`

### ğŸ’¡ Why This Matters
Client behavior is part of system correctness. Controlled retries reduce cascading failures. It unlocks idempotent client semantics in Month 3.

### ğŸ§  Self-Check
- [ ] Why bound retries?
- [ ] What should never be retried?
- [ ] What signal ends the retry loop?

</details>

<details>
<summary>ğŸ—¡ï¸ <b>Day 6 (Sat) â€” BOSS FIGHT: Integrate & Measure</b> â±ï¸ 4h</summary>

### ğŸ“– Learn (40 min)
**Reuse and integration discipline**

Key takeaways:
1. Shared logger reuse
2. Structured network logs
3. Baseline throughput measures

### ğŸ”¨ Do (180 min)
Integrate logger from Week 1 into TCP tools and run echo workload plan.

> ğŸ†• **New constraint:** Each request must carry a traceable request ID in logs.

### âœ… Prove (40 min)
Capture latency and success-rate baseline from 3 payload sizes.

### ğŸ“¦ Ship
`week-2/day6-echo-baseline-report.md`

### ğŸ’¡ Why This Matters
This day proves composition, not restart-from-zero. Measured baselines make future optimizations meaningful. It unlocks multi-client event-loop comparison next week.

### ğŸ§  Self-Check
- [ ] What was reused?
- [ ] What baseline numbers matter?
- [ ] Why attach request IDs now?

</details>

---

## ğŸ“— Chapter 3 â€” Multi-Client Event Loop (Week 3)

> **ğŸ¯ Theme:** `select`/`poll` â€” concurrency without threads.
>
> **ğŸ†• New skill:** Non-blocking state machines
> **ğŸ”„ Reinforcement:** Framing, timeouts, and logging

<details>
<summary>ğŸ—“ï¸ <b>Day 1 (Mon) â€” Quest: Event Loop State Model</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Non-blocking I/O semantics**

Key takeaways:
1. Readiness â‰  completion
2. `EAGAIN` is normal
3. Per-connection state needed

### ğŸ”¨ Do (80 min)
Define connection state model for event loop.

> ğŸ†• **New constraint:** No blocking calls allowed in loop path.

### âœ… Prove (20 min)
Checklist validating all loop operations are non-blocking-safe.

### ğŸ“¦ Ship
`week-3/day1-event-loop-state-model.md`

### ğŸ’¡ Why This Matters
Event loops fail when state is implicit. You make state explicit before scaling connections. It unlocks predictable multi-client behavior.

### ğŸ§  Self-Check
- [ ] Why is `EAGAIN` expected?
- [ ] What state must each connection track?
- [ ] What call can accidentally block?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 2 (Tue) â€” Quest: First Multi-Client Loop</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**`select` mechanics and limits**

Key takeaways:
1. fd sets mutate
2. Max fd caveats
3. Read/write readiness sets

### ğŸ”¨ Do (80 min)
Plan first multi-client loop using `select`.

> ğŸ†• **New constraint:** Support at least 50 concurrent idle clients.

### âœ… Prove (20 min)
Connection matrix test plan (connect/disconnect bursts).

### ğŸ“¦ Ship
`week-3/day2-select-plan.md`

### ğŸ’¡ Why This Matters
This is your first true multi-client architecture step. Even simple loads expose state bugs fast. It unlocks migration to better pollers.

### ğŸ§  Self-Check
- [ ] What are `select` limits?
- [ ] Why track max fd?
- [ ] What events need separate handling?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 3 (Wed) â€” Quest: Backpressure Policy</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Backpressure at socket level**

Key takeaways:
1. Write buffers can fill
2. Slow clients hurt everyone
3. Bounded queues protect process

### ğŸ”¨ Do (80 min)
Define per-client outbound buffer policy.

> ğŸ†• **New constraint:** Cap queued bytes per client and disconnect on abuse.

### âœ… Prove (20 min)
Slow-reader test scenario with expected disconnect threshold.

### ğŸ“¦ Ship
`week-3/day3-backpressure-policy.md`

### ğŸ’¡ Why This Matters
This day prevents one slow client from destabilizing the service. It introduces fairness as a correctness property. It unlocks formal backpressure controls in Month 2.

### ğŸ§  Self-Check
- [ ] Why cap per-client queue?
- [ ] What is fairness in I/O servers?
- [ ] When is forced disconnect correct?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 4 (Thu) â€” Quest: Poll Migration</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**`poll` advantages over `select`**

Key takeaways:
1. Dynamic fd list
2. Simpler scaling
3. Cleaner event iteration

### ğŸ”¨ Do (80 min)
Plan migration from `select` to `poll`.

> ğŸ†• **New constraint:** Preserve exact protocol behavior while changing poller.

### âœ… Prove (20 min)
Regression checklist comparing outputs before/after migration.

### ğŸ“¦ Ship
`week-3/day4-poll-migration-checklist.md`

### ğŸ’¡ Why This Matters
You practice changing internals without changing behavior. This is key for long-lived systems. It unlocks future epoll upgrade with confidence.

### ğŸ§  Self-Check
- [ ] What behavior must stay identical?
- [ ] Why swap poller now?
- [ ] How do you detect regression quickly?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 5 (Fri) â€” Quest: Connection Lifecycle Tests</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Connection churn handling**

Key takeaways:
1. Half-close states
2. Cleanup ordering
3. fd leak detection

### ğŸ”¨ Do (80 min)
Define lifecycle for open/read/write/error/close paths.

> ğŸ†• **New constraint:** Zero descriptor leaks under churn.

### âœ… Prove (20 min)
Run a churn script plan with leak counter target.

### ğŸ“¦ Ship
`week-3/day5-connection-lifecycle-tests.md`

### ğŸ’¡ Why This Matters
Resource leaks kill long-running services quietly. This day adds operational durability, not just correctness. It unlocks safe long soak tests.

### ğŸ§  Self-Check
- [ ] What is half-close?
- [ ] How do fd leaks appear?
- [ ] What cleanup order is safest?

</details>

<details>
<summary>ğŸ—¡ï¸ <b>Day 6 (Sat) â€” BOSS FIGHT: 30-Min Soak Test</b> â±ï¸ 4h</summary>

### ğŸ“– Learn (40 min)
**Load-testing basics**

Key takeaways:
1. Throughput vs latency
2. Percentile thinking
3. Bottleneck classification

### ğŸ”¨ Do (180 min)
Plan and run multi-client soak test design.

> ğŸ†• **New constraint:** Maintain service correctness for 30 minutes under sustained load.

### âœ… Prove (40 min)
Collect p50/p95 latency and error rate over time.

### ğŸ“¦ Ship
`week-3/day6-soak-report.md`

### ğŸ’¡ Why This Matters
This is first endurance check. It reveals memory and lifecycle faults hidden in short tests. It unlocks confidence before adding HTTP behavior.

### ğŸ§  Self-Check
- [ ] Which latency percentile matters most here?
- [ ] What failure appeared first?
- [ ] What metric suggests memory/resource issues?

</details>

---

## ğŸ“— Chapter 4 â€” Epoll & HTTP Client (Week 4)

> **ğŸ¯ Theme:** Connect socket mechanics to real protocol interactions.
>
> **ğŸ†• New skill:** `epoll`/timers + HTTP parsing
> **ğŸ”„ Reinforcement:** Framing/timeouts/error contracts

<details>
<summary>ğŸ—“ï¸ <b>Day 1 (Mon) â€” Quest: Epoll Strategy</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**`epoll` model**

Key takeaways:
1. Registration lifecycle
2. Edge vs level triggers
3. Wakeup efficiency

### ğŸ”¨ Do (80 min)
Define epoll event strategy for your server.

> ğŸ†• **New constraint:** Choose trigger mode and justify starvation prevention.

### âœ… Prove (20 min)
Event-handling invariants checklist for missed-read prevention.

### ğŸ“¦ Ship
`week-4/day1-epoll-strategy.md`

### ğŸ’¡ Why This Matters
Efficient event notification matters as concurrency grows. This day hardens your architecture decisions before coding complexity rises. It unlocks timer-driven cleanup.

### ğŸ§  Self-Check
- [ ] Edge vs level tradeoff?
- [ ] How avoid missed events?
- [ ] What must happen after readiness?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 2 (Tue) â€” Quest: Timer Design</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Timer integration with event loops**

Key takeaways:
1. Idle timeout wheel/heap
2. Timer drift awareness
3. Cleanup scheduling

### ğŸ”¨ Do (80 min)
Define idle timeout and periodic health checks.

> ğŸ†• **New constraint:** Stale connections removed without scanning all clients each tick.

### âœ… Prove (20 min)
Timeout-accuracy measurement plan.

### ğŸ“¦ Ship
`week-4/day2-timer-design.md`

### ğŸ’¡ Why This Matters
Timers prevent silent resource hoarding. This day adds temporal correctness to your server. It unlocks robust slow-client control.

### ğŸ§  Self-Check
- [ ] Why avoid full scans?
- [ ] What timer accuracy is acceptable?
- [ ] How log timeout reasons?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 3 (Wed) â€” Quest: HTTP Parser Spec</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**HTTP/1.1 essentials for clients**

Key takeaways:
1. Request line + headers
2. Status code families
3. `Content-Length` parsing

### ğŸ”¨ Do (80 min)
Specify simple HTTP client request/response parser behavior.

> ğŸ†• **New constraint:** Reject malformed headers with explicit error class.

### âœ… Prove (20 min)
Build parser test list for normal and malformed responses.

### ğŸ“¦ Ship
`week-4/day3-http-parser-spec.md`

### ğŸ’¡ Why This Matters
HTTP gives practical protocol parsing experience beyond echo. This day strengthens input-safety habits. It unlocks health-check integrations for later services.

### ğŸ§  Self-Check
- [ ] What is minimal valid HTTP response?
- [ ] How detect malformed headers?
- [ ] Why parse `Content-Length` carefully?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 4 (Thu) â€” Quest: HTTP Timeout Matrix</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**DNS/connect timeout behavior**

Key takeaways:
1. Connection phases
2. Timeout per phase
3. Distinguish transient vs permanent failures

### ğŸ”¨ Do (80 min)
Define HTTP client timeout and retry policy.

> ğŸ†• **New constraint:** Separate connect timeout from read timeout.

### âœ… Prove (20 min)
Failure matrix for unreachable host, slow server, partial response.

### ğŸ“¦ Ship
`week-4/day4-http-timeout-matrix.md`

### ğŸ’¡ Why This Matters
Separate timeout classes improve diagnosis and resilience. This mirrors real production client behavior. It unlocks robust node-to-node RPC later.

### ğŸ§  Self-Check
- [ ] Why separate connect/read timeout?
- [ ] Which failures are retryable?
- [ ] How should retry budget be set?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 5 (Fri) â€” Quest: End-to-End Trace</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Integration testing with local endpoints**

Key takeaways:
1. Deterministic fixtures
2. Request IDs across client/server
3. Reproducible logs

### ğŸ”¨ Do (80 min)
Define tests where HTTP client queries your server health endpoint.

> ğŸ†• **New constraint:** Consistent correlation ID across both tools.

### âœ… Prove (20 min)
End-to-end trace from request to server response log.

### ğŸ“¦ Ship
`week-4/day5-e2e-trace.md`

### ğŸ’¡ Why This Matters
End-to-end visibility is a systems superpower. This day links independent components through shared observability. It unlocks easier multi-node debugging next month.

### ğŸ§  Self-Check
- [ ] What was traced end-to-end?
- [ ] Why correlation ID matters?
- [ ] Which logs were required to debug one request?

</details>

<details>
<summary>ğŸ—¡ï¸ <b>Day 6 (Sat) â€” ARC BOSS: Month 1 Demo</b> â±ï¸ 4h ğŸ†</summary>

### ğŸ“– Learn (40 min)
**Month synthesis and gap analysis**

Key takeaways:
1. Architecture map
2. Bottleneck list
3. Reliability debt backlog

### ğŸ”¨ Do (180 min)
Build Month 1 integrated demo (CLI + echo server + event loop + HTTP client).

> ğŸ†• **New constraint:** Demo must include one induced failure and recovery behavior.

### âœ… Prove (40 min)
Capture baseline metrics and demo checklist completion.

### ğŸ“¦ Ship
`month-1-demo/README.md` + `month-1-demo/diagram.png` + `week-4/day6-month1-report.md`

### ğŸ† Achievement Unlocked: **Byte Wrangler**
> *You built a multi-client event-driven server with framing, timeouts, and observability from scratch.*

### ğŸ’¡ Why This Matters
You close Month 1 with a coherent system, not fragments. Failure demonstration proves you understand behavior under stress. It unlocks concurrency and crypto work with a stable base.

### ğŸ§  Self-Check
- [ ] Which component is weakest now?
- [ ] What failure did you induce?
- [ ] What metric baseline carries into Month 2?

</details>

---

# âš”ï¸ Arc 2 â€” Hardening (Month 2)

> **ğŸ¯ Mission:** Add concurrency control and cryptographic trust primitives.
>
> **ğŸ§  Mindset unlock:** *"Correctness under load needs both scheduling discipline and cryptographic guarantees."*

```
Month 2 Progress
[â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0% â€” Week 5 Â· 6 Â· 7 Â· 8
```

---

## ğŸ“• Chapter 5 â€” Thread Pool & Safe Task Execution (Week 5)

> **ğŸ¯ Theme:** Event loop handles I/O; thread pool handles bounded CPU work.
>
> **ğŸ†• New skill:** Thread synchronization and graceful shutdown
> **ğŸ”„ Reinforcement:** Queue limits and observability

<details>
<summary>ğŸ—“ï¸ <b>Day 1 (Mon) â€” Quest: Concurrency Model</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**C++ threads and shared-state risks**

Key takeaways:
1. Data races are **undefined behavior**
2. Mutex protects invariants
3. Lock scope should be small

### ğŸ”¨ Do (80 min)
Define concurrency model (event loop + worker pool responsibilities).

> ğŸ†• **New constraint:** No shared mutable state without explicit ownership rule.

### âœ… Prove (20 min)
Ownership map of each shared object.

### ğŸ“¦ Ship
`week-5/day1-concurrency-model.md`

### ğŸ’¡ Why This Matters
This prevents ad-hoc locking as complexity grows. Clear ownership is the core of safe concurrency. It unlocks predictable task processing.

### ğŸ§  Self-Check
- [ ] What is a data race?
- [ ] Which state is thread-confined?
- [ ] Which state is shared and why?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 2 (Tue) â€” Quest: Bounded Work Queue</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Producer-consumer queues**

Key takeaways:
1. Condition-variable signaling
2. Spurious wakeups
3. Bounded capacity

### ğŸ”¨ Do (80 min)
Design bounded work queue for worker pool.

> ğŸ†• **New constraint:** Hard max queue depth with explicit rejection behavior.

### âœ… Prove (20 min)
Overload scenario test plan at queue full condition.

### ğŸ“¦ Ship
`week-5/day2-bounded-queue-spec.md`

### ğŸ’¡ Why This Matters
Unbounded queues hide overload until memory collapses. This day makes overload visible and controllable. It unlocks backpressure strategy next week.

### ğŸ§  Self-Check
- [ ] Why bounded queue?
- [ ] What happens when full?
- [ ] What is correct wake-up condition?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 3 (Wed) â€” Quest: Scheduling Policy</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Task scheduling fairness**

Key takeaways:
1. FIFO tradeoffs
2. Starvation risk
3. Task timeouts

### ğŸ”¨ Do (80 min)
Define task dispatch rules for CPU-bound work.

> ğŸ†• **New constraint:** Max task execution budget with cancellation path.

### âœ… Prove (20 min)
Test plan for one long task among many short tasks.

### ğŸ“¦ Ship
`week-5/day3-scheduling-policy.md`

### ğŸ’¡ Why This Matters
Fair scheduling keeps latency stable under mixed workloads. This avoids hidden starvation bugs. It unlocks predictable signing/hash workloads.

### ğŸ§  Self-Check
- [ ] What causes starvation?
- [ ] Why task budget?
- [ ] What should cancellation guarantee?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 4 (Thu) â€” Quest: Contention Metrics</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Contention measurement basics**

Key takeaways:
1. Lock wait time
2. Queue wait time
3. Throughput-latency tradeoff

### ğŸ”¨ Do (80 min)
Define instrumentation points around queue and locks.

> ğŸ†• **New constraint:** Capture p95 queue wait for every task type.

### âœ… Prove (20 min)
Build metric collection checklist and expected ranges.

### ğŸ“¦ Ship
`week-5/day4-contention-metrics.md`

### ğŸ’¡ Why This Matters
Concurrency without measurement is guesswork. This day sets concrete performance evidence. It unlocks objective tuning.

### ğŸ§  Self-Check
- [ ] Which metric reveals contention first?
- [ ] Why p95 over average?
- [ ] What threshold means overload?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 5 (Fri) â€” Quest: Graceful Shutdown</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Graceful shutdown design**

Key takeaways:
1. Stop intake first
2. Drain queue
3. Join workers safely

### ğŸ”¨ Do (80 min)
Define shutdown sequence and deadlines.

> ğŸ†• **New constraint:** Zero task loss for accepted work during graceful shutdown.

### âœ… Prove (20 min)
Shutdown test checklist with in-flight tasks.

### ğŸ“¦ Ship
`week-5/day5-graceful-shutdown.md`

### ğŸ’¡ Why This Matters
Clean shutdown is reliability, not polish. It protects correctness during deploys and crashes. It unlocks safer failure drills.

### ğŸ§  Self-Check
- [ ] What is shutdown order?
- [ ] How avoid task loss?
- [ ] When force-terminate?

</details>

<details>
<summary>ğŸ—¡ï¸ <b>Day 6 (Sat) â€” BOSS FIGHT: Threadpool Benchmark</b> â±ï¸ 4h</summary>

### ğŸ“– Learn (40 min)
**Comparative benchmarking**

Key takeaways:
1. Single-thread baseline
2. Worker-pool scaling curve
3. Diminishing returns

### ğŸ”¨ Do (180 min)
Plan benchmark comparing event-loop-only vs event-loop+pool workloads.

> ğŸ†• **New constraint:** Publish scaling limit and likely bottleneck.

### âœ… Prove (40 min)
Capture throughput and p95 latency for 1/2/4 worker counts.

### ğŸ“¦ Ship
`week-5/day6-threadpool-benchmark.md`

### ğŸ’¡ Why This Matters
You need proof that concurrency helps, not just complexity. This day quantifies tradeoffs. It unlocks informed backpressure tuning.

### ğŸ§  Self-Check
- [ ] Where did scaling flatten?
- [ ] What bottleneck appeared?
- [ ] Which worker count is best and why?

</details>

---

## ğŸ“• Chapter 6 â€” Backpressure & Overload Handling (Week 6)

> **ğŸ¯ Theme:** Survivability under load â€” core distributed-systems behavior.
>
> **ğŸ†• New skill:** Overload policy and slow-client defense
> **ğŸ”„ Reinforcement:** Queue limits and timeouts

<details>
<summary>ğŸ—“ï¸ <b>Day 1 (Mon) â€” Quest: Overload Policy Ladder</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Backpressure patterns**
1. Queue bounds â†’ 2. Credit/token models â†’ 3. Fail-fast responses

### ğŸ”¨ Do (80 min)
Define server overload policy ladder.
> ğŸ†• **New constraint:** Explicit reject mode when queue depth crosses threshold.

### âœ… Prove (20 min)
Threshold table with expected client-visible behavior.

### ğŸ“¦ Ship
`week-6/day1-overload-policy.md`

### ğŸ’¡ Why
Backpressure turns chaos into controlled degradation.

### ğŸ§  Self-Check
- [ ] Why fail fast? Â· What thresholds define overload? Â· How should clients respond to rejection?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 2 (Tue) â€” Quest: Slow-Client Defense</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Slow-client attack patterns** â€” slowloris, read deadlines, per-connection quotas

### ğŸ”¨ Do (80 min)
Define slow-client defense strategy.
> ğŸ†• **New constraint:** Minimum progress rule for active connections.

### âœ… Prove (20 min)
Simulated slow-sender test plan and expected disconnect timing.

### ğŸ“¦ Ship
`week-6/day2-slow-client-defense.md`

### ğŸ’¡ Why
One bad peer can starve resources without this control.

### ğŸ§  Self-Check
- [ ] What is minimum progress rule? Â· Why not allow infinite slow sends? Â· What metric signals abuse?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 3 (Wed) â€” Quest: Egress Throttle</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Write-side pressure control** â€” socket send buffer limits, app-level buffer caps, drop/close policy

### ğŸ”¨ Do (80 min)
Define outbound throttling behavior.
> ğŸ†• **New constraint:** Per-client egress rate limit with burst cap.

### âœ… Prove (20 min)
High-volume client test case verifying throttling kicks in.

### ğŸ“¦ Ship
`week-6/day3-egress-throttle.md`

### ğŸ’¡ Why
Protects server memory and fairness. Prepares you for replication traffic shaping.

### ğŸ§  Self-Check
- [ ] Why rate-limit writes? Â· What is burst cap? Â· When close vs throttle?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 4 (Thu) â€” Quest: Deadline Budget</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Tail latency management** â€” p99 pain point, deadline propagation, timeout budget split

### ğŸ”¨ Do (80 min)
Define request deadline budget across stages.
> ğŸ†• **New constraint:** Drop request when deadline is exceeded at any stage.

### âœ… Prove (20 min)
Deadline violation scenario with expected logs.

### ğŸ“¦ Ship
`week-6/day4-deadline-budget.md`

### ğŸ’¡ Why
Deadlines prevent zombie work and long-tail collapse.

### ğŸ§  Self-Check
- [ ] Why p99 matters? Â· How split deadlines? Â· What should happen on expired deadline?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 5 (Fri) â€” Quest: Failure Injection Matrix</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Failure injection basics** â€” deliberate stress, controlled hypotheses, observable outcomes

### ğŸ”¨ Do (80 min)
Plan overload + slow-client failure drill matrix.
> ğŸ†• **New constraint:** Every drill must map to one quality gate metric.

### âœ… Prove (20 min)
Define expected fail-safe behavior for 5 stress scenarios.

### ğŸ“¦ Ship
`week-6/day5-failure-injection-matrix.md`

### ğŸ’¡ Why
You build muscle for "break it on purpose." Trust systems avoid surprise collapses.

### ğŸ§  Self-Check
- [ ] What is a good failure drill? Â· Which metric defines pass? Â· Why test fail-safe behavior early?

</details>

<details>
<summary>ğŸ—¡ï¸ <b>Day 6 (Sat) â€” BOSS FIGHT: Backpressure Report</b> â±ï¸ 4h</summary>

### ğŸ”¨ Do (180 min)
Run overload experiments and finalize backpressure thresholds.
> ğŸ†• **New constraint:** Maintain defined error-rate cap under target load.

### âœ… Prove (40 min)
Publish throughput/latency/error chart for normal vs overload.

### ğŸ“¦ Ship
`week-6/day6-backpressure-report.md`

### ğŸ§  Self-Check
- [ ] Which threshold is most sensitive? Â· Did error-rate cap hold? Â· What tradeoff did you accept?

</details>

---

## ğŸ“• Chapter 7 â€” Hashing & Integrity Proofs (Week 7)

> **ğŸ¯ Theme:** Tamper detection starts with digest correctness.
>
> **ğŸ†• New skill:** Cryptographic integrity
> **ğŸ”„ Reinforcement:** Streaming and framing constraints

<details>
<summary>ğŸ—“ï¸ <b>Day 1 (Mon) â€” Quest: Hash Use Cases</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Hash fundamentals** â€” preimage/collision concepts, integrity use, non-secret vs secret primitives

### ğŸ”¨ Do (80 min)
Define hash-tool use cases (files, payloads, logs).
> ğŸ†• **New constraint:** Canonical byte representation before hashing.

### âœ… Prove (20 min)
Cross-platform hash consistency test cases.

### ğŸ“¦ Ship
`week-7/day1-hash-use-cases.md`

### ğŸ’¡ Why
Hashing only helps if bytes are canonical. Prevents false mismatches in signatures later.

### ğŸ§  Self-Check
- [ ] Why canonicalization first? Â· What is collision risk in practice? Â· Where should hash be stored?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 2 (Tue) â€” Quest: Streaming Hash Plan</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Streaming digest computation** â€” chunked updates, large-file memory safety, finalization

### ğŸ”¨ Do (80 min)
Define incremental hash workflow for large payloads.
> ğŸ†• **New constraint:** No full-file load into memory.

### âœ… Prove (20 min)
Large-input test plan with memory cap target.

### ğŸ“¦ Ship
`week-7/day2-streaming-hash-plan.md`

### ğŸ§  Self-Check
- [ ] Why incremental hashing? Â· What memory cap is acceptable? Â· How verify same digest as one-shot?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 3 (Wed) â€” Quest: Protocol Hash Envelope</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Integrity in protocols** â€” payload hash field, mismatch handling, logging forensic context

### ğŸ”¨ Do (80 min)
Add hash field to protocol envelope spec.
> ğŸ†• **New constraint:** Reject and audit any hash mismatch.

### âœ… Prove (20 min)
Tampered-payload scenario with expected reject reason.

### ğŸ“¦ Ship
`week-7/day3-protocol-hash-envelope.md`

### ğŸ’¡ Why
Upgrades protocol from transport-only to integrity-aware. Makes tampering visible.

### ğŸ§  Self-Check
- [ ] What does payload hash protect? Â· What does it NOT protect? Â· How should mismatch be reported?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 4 (Thu) â€” Quest: Canonicalization Rules</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Hash misuse pitfalls** â€” hashing mutable forms, ambiguous encodings, wrong digest context

### ğŸ”¨ Do (80 min)
Define canonicalization and encoding rules for all hash inputs.
> ğŸ†• **New constraint:** Single canonical serialization for signed/hashed data.

### âœ… Prove (20 min)
Canonicalization regression cases (field order, whitespace, line endings).

### ğŸ“¦ Ship
`week-7/day4-canonicalization-rules.md`

### ğŸ’¡ Why
Most signature bugs are serialization bugs. Prevents cross-node verification failures.

### ğŸ§  Self-Check
- [ ] What fields must be canonicalized? Â· Why can whitespace break trust? Â· How lock serialization format?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 5 (Fri) â€” Quest: Integrity Audit Drill</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Integrity audit workflows** â€” periodic scan, quarantine strategy, audit trail

### ğŸ”¨ Do (80 min)
Define hash-audit process for stored artifacts.
> ğŸ†• **New constraint:** Audit must detect silent file corruption.

### âœ… Prove (20 min)
Corrupt-one-byte drill and expected detection log.

### ğŸ“¦ Ship
`week-7/day5-integrity-audit-drill.md`

### ğŸ§  Self-Check
- [ ] How often audit? Â· What happens on mismatch? Â· What evidence is preserved?

</details>

<details>
<summary>ğŸ—¡ï¸ <b>Day 6 (Sat) â€” BOSS FIGHT: Hash Integration</b> â±ï¸ 4h</summary>

### ğŸ”¨ Do (180 min)
Integrate hash tool and protocol digest fields into existing stack.
> ğŸ†• **New constraint:** Protocol versioning for backward compatibility.

### âœ… Prove (40 min)
Run compatibility tests between old/new message formats.

### ğŸ“¦ Ship
`week-7/day6-hash-integration-report.md`

### ğŸ§  Self-Check
- [ ] Why version protocol now? Â· What compatibility break did you avoid? Â· Which old clients still work?

</details>

---

## ğŸ“• Chapter 8 â€” Signatures & Replay Protection (Week 8)

> **ğŸ¯ Theme:** Identity and anti-replay are core trust-system primitives.
>
> **ğŸ†• New skill:** Key lifecycle and signature verification
> **ğŸ”„ Reinforcement:** Canonicalization and protocol envelopes

<details>
<summary>ğŸ—“ï¸ <b>Day 1 (Mon) â€” Quest: Key Policy</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Key lifecycle basics** â€” generation, storage permissions, rotation and revocation

### ğŸ”¨ Do (80 min)
Define key management policy for local dev system.
> ğŸ†• **New constraint:** Key files must have restricted permissions and versioned key IDs.

### âœ… Prove (20 min)
Key-file permission and key-ID mapping checklist.

### ğŸ“¦ Ship
`week-8/day1-key-policy.md`

### ğŸ§  Self-Check
- [ ] Why key IDs? Â· What permission model is required? Â· When rotate keys?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 2 (Tue) â€” Quest: Sign/Verify Spec</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Signature verification flow** â€” sign canonical bytes, verify before processing, fail closed

### ğŸ”¨ Do (80 min)
Define sign/verify CLI behavior and errors.
> ğŸ†• **New constraint:** Reject any unsigned or unverifiable message by default.

### âœ… Prove (20 min)
Invalid-signature test matrix (wrong key, altered payload, altered metadata).

### ğŸ“¦ Ship
`week-8/day2-sign-verify-spec.md`

### ğŸ’¡ Why
This is your first identity-bound protocol enforcement. Changes trust from "maybe honest" to **verifiable**.

### ğŸ§  Self-Check
- [ ] What exactly is signed? Â· Why verify before processing? Â· What is fail-closed behavior?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 3 (Wed) â€” Quest: Replay Defense</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Replay attacks and nonce design** â€” old valid message replay, nonce uniqueness, timestamp window

### ğŸ”¨ Do (80 min)
Define replay-defense policy with nonce cache + time window.
> ğŸ†• **New constraint:** Duplicate `(key_id, nonce)` is ALWAYS rejected.

### âœ… Prove (20 min)
Replay test where same signed packet is resent 3 times.

### ğŸ“¦ Ship
`week-8/day3-replay-policy.md`

### ğŸ’¡ Why
Integrity alone does NOT stop replay. You need temporal + uniqueness constraints.

### ğŸ§  Self-Check
- [ ] Why signatures don't stop replay? Â· How long keep nonce cache? Â· What about clock skew?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 4 (Thu) â€” Quest: Signed Envelope v1</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Envelope versioning and compatibility** â€” signed header fields, extensibility, deprecation

### ğŸ”¨ Do (80 min)
Finalize signed protocol envelope schema.
> ğŸ†• **New constraint:** Include protocol version and mandatory signed metadata.

### âœ… Prove (20 min)
Version compatibility scenarios and expected outcomes.

### ğŸ“¦ Ship
`week-8/day4-signed-envelope-v1.md`

### ğŸ§  Self-Check
- [ ] Which headers must be signed? Â· Why include version in signature? Â· How handle unknown version?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 5 (Fri) â€” Quest: Verify Performance</b> â±ï¸ 2h</summary>

### ğŸ“– Learn (30 min)
**Verification performance and caching** â€” key cache, signature verify cost, rejection fast path

### ğŸ”¨ Do (80 min)
Define verification pipeline optimization plan.
> ğŸ†• **New constraint:** Cap verification latency while preserving fail-closed semantics.

### âœ… Prove (20 min)
Measure expected verification cost for small vs large payloads.

### ğŸ“¦ Ship
`week-8/day5-verify-performance.md`

### ğŸ§  Self-Check
- [ ] Where is verify bottleneck? Â· What can be cached safely? Â· What must NEVER bypass verification?

</details>

<details>
<summary>ğŸ—¡ï¸ <b>Day 6 (Sat) â€” ARC BOSS: Signed Protocol Demo</b> â±ï¸ 4h ğŸ†</summary>

### ğŸ”¨ Do (180 min)
Build integrated signed protocol demo (client-server).
> ğŸ†• **New constraint:** Full request path enforces signature + replay + timeout.

### âœ… Prove (40 min)
Capture pass/fail evidence for valid, tampered, replayed, and expired requests.

### ğŸ“¦ Ship
`month-2-demo/README.md` + `week-8/day6-signed-protocol-report.md`

### ğŸ† Achievement Unlocked: **Trust Forger**
> *You built a signed, replay-protected network protocol with overload controls.*

### ğŸ§  Self-Check
- [ ] Which attacks are now blocked? Â· What still is NOT covered? Â· Which log proves replay defense worked?

</details>

---

# ğŸ° Arc 3 â€” Distributed Core (Month 3)

> **ğŸ¯ Mission:** Build distributed core with durability, replication, and failure-tolerant clients.
>
> **ğŸ§  Mindset unlock:** *"Correctness means surviving crashes, retries, and partial failures."*

```
Month 3 Progress
[â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0% â€” Week 9 Â· 10 Â· 11 Â· 12
```

---

## ğŸ“™ Chapter 9 â€” KV Store Core & State Model (Week 9)

> **ğŸ†• New skill:** State-machine design
> **ğŸ”„ Reinforcement:** Protocol contracts and request IDs

<details>
<summary>ğŸ—“ï¸ <b>Day 1 (Mon) â€” Quest: KV Command Spec</b> â±ï¸ 2h</summary>

### ğŸ“– Learn â€” KV state machine basics: commands mutate state, reads are deterministic, invalid ops are explicit
### ğŸ”¨ Do â€” Define `put/get/delete` + response schema. **Constraint:** Every mutating command requires unique request ID.
### âœ… Prove â€” Command validity matrix including missing keys and duplicate IDs.
### ğŸ“¦ Ship â€” `week-9/day1-kv-command-spec.md`
### ğŸ§  Self-Check
- [ ] Why request IDs on writes? Â· How should missing keys respond? Â· What makes command deterministic?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 2 (Tue) â€” Quest: Versioning Rules</b> â±ï¸ 2h</summary>

### ğŸ“– Learn â€” In-memory indexing and versioning: version counters, optimistic conflict awareness, metadata separation
### ğŸ”¨ Do â€” Define key metadata model including version and last-update term. **Constraint:** Version increments on every successful write.
### âœ… Prove â€” Concurrent-write scenario expectations table.
### ğŸ“¦ Ship â€” `week-9/day2-versioning-rules.md`
### ğŸ§  Self-Check
- [ ] Why track version? Â· What is stale write? Â· Which metadata will replication need?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 3 (Wed) â€” Quest: Serialization Format</b> â±ï¸ 2h</summary>

### ğŸ“– Learn â€” Serialization design: stable field ordering, forward compatibility, checksums
### ğŸ”¨ Do â€” Define binary/text record format for snapshot and logs. **Constraint:** Include checksum for each persisted record.
### âœ… Prove â€” Corrupted-record detection test plan.
### ğŸ“¦ Ship â€” `week-9/day3-serialization-format.md`
### ğŸ§  Self-Check
- [ ] Why checksum each record? Â· What breaks forward compatibility? Â· How detect decode errors safely?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 4 (Thu) â€” Quest: Snapshot Rules</b> â±ï¸ 2h</summary>

### ğŸ“– Learn â€” Snapshot strategy: point-in-time copy, atomic replace, metadata headers
### ğŸ”¨ Do â€” Define snapshot creation and load rules. **Constraint:** Snapshot apply only if checksum and schema version pass.
### âœ… Prove â€” Snapshot corruption scenario and expected fallback behavior.
### ğŸ“¦ Ship â€” `week-9/day4-snapshot-rules.md`
### ğŸ§  Self-Check
- [ ] Why need snapshots if WAL exists? Â· When reject snapshot? Â· What metadata is mandatory?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 5 (Fri) â€” Quest: KV Concurrency Policy</b> â±ï¸ 2h</summary>

### ğŸ“– Learn â€” Concurrency in state machines: serialize writes, read consistency, lock granularity
### ğŸ”¨ Do â€” Define concurrency policy for KV operations. **Constraint:** Single-writer discipline to preserve ordering.
### âœ… Prove â€” Race test design with concurrent reads/writes.
### ğŸ“¦ Ship â€” `week-9/day5-kv-concurrency-policy.md`
### ğŸ§  Self-Check
- [ ] Why single-writer now? Â· What read consistency is acceptable? Â· Where can parallelism remain?

</details>

<details>
<summary>ğŸ—¡ï¸ <b>Day 6 (Sat) â€” BOSS FIGHT: KV + Signed Integration</b> â±ï¸ 4h</summary>

### ğŸ”¨ Do â€” Compose signed request handling with KV command execution. **Constraint:** Reject unsigned state-changing commands.
### âœ… Prove â€” End-to-end signed `put/get/delete` scenario evidence.
### ğŸ“¦ Ship â€” `week-9/day6-kv-signed-integration.md`
### ğŸ§  Self-Check
- [ ] Which commands require signature? Â· How is request ID propagated? Â· What evidence proves integration works?

</details>

---

## ğŸ“™ Chapter 10 â€” WAL Durability & Crash Recovery (Week 10)

> **ğŸ†• New skill:** Write-ahead logging + recovery discipline
> **ğŸ”„ Reinforcement:** Checksums and deterministic command model

<details>
<summary>ğŸ—“ï¸ <b>Day 1 (Mon) â€” Quest: WAL Schema</b> â±ï¸ 2h</summary>

### ğŸ“– Learn â€” WAL principles: append before apply, durable ordering, fsync policy tradeoff
### ğŸ”¨ Do â€” Define WAL record schema and append sequence. **Constraint:** State apply ONLY after WAL append success.
### âœ… Prove â€” Sequence-of-events checklist for each command.
### ğŸ“¦ Ship â€” `week-10/day1-wal-schema.md`
### ğŸ§  Self-Check
- [ ] Why append-before-apply? Â· What if append fails? Â· Which fields must WAL include?

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 2 (Tue) â€” Quest: Fsync Policy</b> â±ï¸ 2h</summary>

### ğŸ“– Learn â€” Durability levels: sync every write, batch sync, risk/performance tradeoff
### ğŸ”¨ Do â€” Define fsync policy by command class. **Constraint:** Critical writes require immediate sync mode.
### âœ… Prove â€” Durability policy table with expected latency impact.
### ğŸ“¦ Ship â€” `week-10/day2-fsync-policy.md`

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 3 (Wed) â€” Quest: Crash Drill Procedure</b> â±ï¸ 2h</summary>

### ğŸ“– Learn â€” Crash simulation: abrupt termination, partial write risks, restart verification
### ğŸ”¨ Do â€” Define crash-drill procedure around write operations. **Constraint:** Must detect and ignore torn/corrupt WAL tail.
### âœ… Prove â€” Crash case matrix: before append, after append, after apply.
### ğŸ“¦ Ship â€” `week-10/day3-crash-drill-procedure.md`

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 4 (Thu) â€” Quest: Recovery Algorithm</b> â±ï¸ 2h</summary>

### ğŸ“– Learn â€” Recovery replay: idempotent replay, checksum validation, replay cutoff rules
### ğŸ”¨ Do â€” Define startup replay algorithm and validation gates. **Constraint:** Stop replay at first invalid record and quarantine remainder.
### âœ… Prove â€” Recovery scenario walkthrough with expected final state.
### ğŸ“¦ Ship â€” `week-10/day4-recovery-algorithm.md`

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 5 (Fri) â€” Quest: Checkpoint & Compaction</b> â±ï¸ 2h</summary>

### ğŸ“– Learn â€” Checkpoint and compaction: reduce replay time, consistent cut, WAL truncation safety
### ğŸ”¨ Do â€” Define checkpoint trigger policy and truncation rules. **Constraint:** Truncate ONLY after verified checkpoint durability.
### âœ… Prove â€” Checkpoint/truncate invariants checklist.
### ğŸ“¦ Ship â€” `week-10/day5-checkpoint-compaction.md`

</details>

<details>
<summary>ğŸ—¡ï¸ <b>Day 6 (Sat) â€” BOSS FIGHT: Durability Report</b> â±ï¸ 4h</summary>

### ğŸ”¨ Do â€” Run crash/restart benchmark scenarios for WAL modes. **Constraint:** Define and meet target RTO for restart.
### âœ… Prove â€” Publish durability vs latency table and recovery timings.
### ğŸ“¦ Ship â€” `week-10/day6-durability-report.md`
### ğŸ§  Self-Check
- [ ] What RTO did you meet? Â· Which mode has best balance? Â· Did any committed write get lost?

</details>

---

## ğŸ“™ Chapter 11 â€” Replicated KV (2â€“3 Nodes) (Week 11)

> **ğŸ†• New skill:** Replication protocol and quorum logic
> **ğŸ”„ Reinforcement:** WAL ordering and signed envelopes

<details>
<summary>ğŸ—“ï¸ <b>Day 1 (Mon) â€” Quest: Failure Model</b> â±ï¸ 2h</summary>

### ğŸ“– Learn â€” Failure models: crash-stop, message delay/loss, no Byzantine assumption yet
### ğŸ”¨ Do â€” Define replication assumptions and node roles. **Constraint:** Explicit model excludes Byzantine peers for this phase.
### ğŸ“¦ Ship â€” `week-11/day1-failure-model.md`

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 2 (Tue) â€” Quest: Append RPC Spec</b> â±ï¸ 2h</summary>

### ğŸ“– Learn â€” Leader-to-follower log shipping: append entries RPC, prev-index consistency check, ack semantics
### ğŸ”¨ Do â€” Define append RPC fields and follower validation. **Constraint:** Follower rejects append with mismatched previous index/term.
### ğŸ“¦ Ship â€” `week-11/day2-append-rpc-spec.md`

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 3 (Wed) â€” Quest: Quorum Commit Rules</b> â±ï¸ 2h</summary>

### ğŸ“– Learn â€” Quorum commit: majority acknowledgment, commit index advancement, stale acks
### ğŸ”¨ Do â€” Define commit criteria for 3-node cluster. **Constraint:** Leader applies entry ONLY after majority acks.
### ğŸ“¦ Ship â€” `week-11/day3-quorum-commit-rules.md`

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 4 (Thu) â€” Quest: Follower Catch-Up</b> â±ï¸ 2h</summary>

### ğŸ“– Learn â€” Retry from lower index, conflict resolution, snapshot install fallback
### ğŸ”¨ Do â€” Define catch-up sequence for lagging follower. **Constraint:** Bounded retry steps before snapshot fallback.
### ğŸ“¦ Ship â€” `week-11/day4-follower-catchup.md`

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 5 (Fri) â€” Quest: Partition Policy</b> â±ï¸ 2h</summary>

### ğŸ“– Learn â€” Partition behavior: split-brain risk, minority isolation, recovery sequencing
### ğŸ”¨ Do â€” Define partition behavior policy. **Constraint:** Minority side CANNOT accept committed writes.
### ğŸ“¦ Ship â€” `week-11/day5-partition-policy.md`

</details>

<details>
<summary>ğŸ—¡ï¸ <b>Day 6 (Sat) â€” BOSS FIGHT: Replication Validation</b> â±ï¸ 4h</summary>

### ğŸ”¨ Do â€” Run replicated KV validation plan across 2â€“3 nodes. **Constraint:** Identical committed state after node restart and catch-up.
### âœ… Prove â€” Publish state-hash comparison across nodes after drills.
### ğŸ“¦ Ship â€” `week-11/day6-replication-validation.md`

</details>

---

## ğŸ“™ Chapter 12 â€” Leader Election + Client Idempotency (Week 12)

> **ğŸ†• New skill:** Term-based leader election and idempotent retries
> **ğŸ”„ Reinforcement:** Request IDs and quorum semantics

<details>
<summary>ğŸ—“ï¸ <b>Day 1 (Mon) â€” Quest: Election Timeouts</b> â±ï¸ 2h</summary>

### ğŸ“– Learn â€” Randomized timeout reduces split votes, heartbeat cadence, term monotonicity
### ğŸ”¨ Do â€” Define election timeout/heartbeat ranges. **Constraint:** Randomized election timeout per node.
### ğŸ“¦ Ship â€” `week-12/day1-election-timeouts.md`

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 2 (Tue) â€” Quest: Vote Rules</b> â±ï¸ 2h</summary>

### ğŸ“– Learn â€” One vote per term, up-to-date log requirement, term update on newer term seen
### ğŸ”¨ Do â€” Define candidate/voter state transitions. **Constraint:** Reject vote if candidate log is stale.
### ğŸ“¦ Ship â€” `week-12/day2-vote-rules.md`

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 3 (Wed) â€” Quest: Client Retry + Idempotency</b> â±ï¸ 2h</summary>

### ğŸ“– Learn â€” Retry under leader changes, same request ID, redirect hints
### ğŸ”¨ Do â€” Define client retry policy for `not_leader` and timeout errors. **Constraint:** All retries reuse original request ID.
### ğŸ“¦ Ship â€” `week-12/day3-client-retry-idempotency.md`

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 4 (Thu) â€” Quest: Dedupe Store Rules</b> â±ï¸ 2h</summary>

### ğŸ“– Learn â€” Store recent request IDs, response replay, expiration policy
### ğŸ”¨ Do â€” Define dedupe store rules and TTL. **Constraint:** Duplicate request returns original response, NOT re-execution.
### ğŸ“¦ Ship â€” `week-12/day4-dedupe-store-rules.md`

</details>

<details>
<summary>ğŸ—“ï¸ <b>Day 5 (Fri) â€” Quest: Stale-Leader Fencing</b> â±ï¸ 2h</summary>

### ğŸ“– Learn â€” Term-based fencing, stale write rejection, role transition logging
### ğŸ”¨ Do â€” Define stale-leader handling path. **Constraint:** Any request carrying old term is rejected.
### ğŸ“¦ Ship â€” `week-12/day5-stale-leader-fencing.md`

</details>

<details>
<summary>ğŸ—¡ï¸ <b>Day 6 (Sat) â€” ARC BOSS: Month 3 Demo</b> â±ï¸ 4h ğŸ†</summary>

### ğŸ”¨ Do â€” Build Month 3 integrated demo (durable replicated KV + retries). **Constraint:** Include crash + failover + retry scenario in one scripted flow.
### âœ… Prove â€” No lost committed writes, no duplicate effects.
### ğŸ“¦ Ship â€” `month-3-demo/README.md` + `week-12/day6-month3-report.md`

### ğŸ† Achievement Unlocked: **Fault Tamer**
> *You built a replicated KV store that survives crashes, leader failures, and retries safely.*

</details>

---

# ğŸ”® Arc 4 â€” Trust Architecture (Month 4)

> **ğŸ¯ Mission:** Build tamper-evident trust architecture (CAS + Merkle + transparency log + monitor).
>
> **ğŸ§  Mindset unlock:** *"Trust is not hidden internals; it is publicly verifiable evidence."*

```
Month 4 Progress
[â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0% â€” Week 13 Â· 14 Â· 15 Â· 16
```

---

## ğŸ“˜ Chapter 13 â€” Content-Addressed Storage (Week 13)

> **ğŸ†• New skill:** Hash-addressed object lifecycle
> **ğŸ”„ Reinforcement:** Canonical hashing and persistence safety

| Day | Quest | Key Constraint | Ship |
|-----|-------|----------------|------|
| Mon | CAS Object Model | Object ID = canonical-hash of normalized bytes | `week-13/day1-cas-object-model.md` |
| Tue | CAS Write Lifecycle | Incomplete writes never appear as valid objects | `week-13/day2-cas-write-lifecycle.md` |
| Wed | Chunk Manifest Spec | Manifest hash commits to chunk order and sizes | `week-13/day3-chunk-manifest-spec.md` |
| Thu | CAS GC Policy | Deletion only for unreachable objects after retention delay | `week-13/day4-cas-gc-policy.md` |
| Fri | CAS Audit Drill | Audit produces machine-parseable discrepancy report | `week-13/day5-cas-audit-drill.md` |
| **Sat** ğŸ—¡ï¸ | **BOSS: CAS + KV Integration** | State mutation rejected if referenced CAS object missing | `week-13/day6-cas-kv-integration.md` |

<details>
<summary>ğŸ“‹ Expand full daily details...</summary>

**Mon â€” CAS Object Model:** Learn CAS principles (address = hash, immutability by design, dedup by identity). Prove with same-content-same-ID test cases.

**Tue â€” CAS Write Lifecycle:** Learn blob persistence (object path mapping, atomic write-then-rename, checksum at rest). Prove with interrupted-write scenario.

**Wed â€” Chunk Manifest Spec:** Learn chunking (fixed vs rolling, manifest object, reassembly). Prove with chunk-reorder tamper detection.

**Thu â€” CAS GC Policy:** Learn garbage collection (reachability roots, mark/sweep, safety window). Prove with reachability audit.

**Fri â€” CAS Audit Drill:** Learn store-level integrity (recalc hash, compare object ID, quarantine). Prove with intentional one-byte corruption.

**Sat â€” Integration:** Define integration path where KV stores document references via CAS IDs. Prove with end-to-end reference validation.

</details>

---

## ğŸ“˜ Chapter 14 â€” Merkle Trees & Inclusion Proofs (Week 14)

> **ğŸ†• New skill:** Proof algorithms
> **ğŸ”„ Reinforcement:** Canonical hashing and CAS identities

| Day | Quest | Key Constraint | Ship |
|-----|-------|----------------|------|
| Mon | Merkle Construction Rules | Deterministic leaf ordering across nodes | `week-14/day1-merkle-construction-rules.md` |
| Tue | Inclusion Proof Format | Proof includes leaf index and tree size | `week-14/day2-inclusion-proof-format.md` |
| Wed | Proof Verifier Rules | Fail-closed on any malformed proof element | `week-14/day3-proof-verifier-rules.md` |
| Thu | Incremental Merkle Plan | Append update must not recompute entire tree | `week-14/day4-incremental-merkle-plan.md` |
| Fri | Adversarial Proof Tests | Stale root proofs explicitly marked unverifiable | `week-14/day5-adversarial-proof-tests.md` |
| **Sat** ğŸ—¡ï¸ | **BOSS: Merkle Performance** | Publish max acceptable proof verification latency | `week-14/day6-merkle-performance-report.md` |

---

## ğŸ“˜ Chapter 15 â€” Transparency Log (Week 15)

> **ğŸ†• New skill:** Consistency proofs and checkpoint signing
> **ğŸ”„ Reinforcement:** Merkle proofs and key management

| Day | Quest | Key Constraint | Ship |
|-----|-------|----------------|------|
| Mon | Log Append Contract | No deletion or in-place mutation of historical entries | `week-15/day1-log-append-contract.md` |
| Tue | Inclusion API Bundle | Every proof response references signed checkpoint | `week-15/day2-inclusion-api-bundle.md` |
| Wed | Consistency Proof Rules | Any new checkpoint must be consistency-provable from previous | `week-15/day3-consistency-proof-rules.md` |
| Thu | Checkpoint Signature Schema | Checkpoint includes monotonic sequence + signing key ID | `week-15/day4-checkpoint-signature-schema.md` |
| Fri | Verifier Workflow | Reject proof lacking checkpoint continuity from cached state | `week-15/day5-verifier-workflow.md` |
| **Sat** ğŸ—¡ï¸ | **BOSS: Auditor Design** | Auditor detects and reports checkpoint inconsistencies | `week-15/day6-auditor-design.md` |

---

## ğŸ“˜ Chapter 16 â€” Monitoring & Anti-Equivocation (Week 16)

> **ğŸ†• New skill:** Monitor gossip and incident response
> **ğŸ”„ Reinforcement:** Checkpoints/proofs/signature verification

| Day | Quest | Key Constraint | Ship |
|-----|-------|----------------|------|
| Mon | Monitor Architecture | Monitor stores immutable observation log | `week-16/day1-monitor-architecture.md` |
| Tue | Monitor Gossip Schema | Gossip includes signed checkpoint + source metadata | `week-16/day2-monitor-gossip-schema.md` |
| Wed | Equivocation Detection | Any conflict generates signed incident record | `week-16/day3-equivocation-detection.md` |
| Thu | Alert Policy | Critical alerts require cryptographic evidence attachment | `week-16/day4-alert-policy.md` |
| Fri | Incident Runbook | Runbook includes freeze-new-acceptance decision criteria | `week-16/day5-incident-runbook.md` |
| **Sat** ğŸ—¡ï¸ | **ARC BOSS: Month 4 Trust Demo** ğŸ† | Include simulated equivocation + monitor response | `month-4-demo/README.md` + `week-16/day6-month4-report.md` |

> ### ğŸ† Achievement Unlocked: **Trust Architect**
> *You built a tamper-evident transparency system with proofs, monitors, and incident detection.*

---

# ğŸ‘‘ Arc 5 â€” CivicTrust Capstone (Month 5)

> **ğŸ¯ Mission:** Compose identity, transparency, verification, and failure handling into one workflow.
>
> **ğŸ§  Mindset unlock:** *"Real systems combine everything into one workflow."*

```
Month 5 Progress
[â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0% â€” Week 17 Â· 18 Â· 19 Â· 20
```

---

## ğŸ““ Chapter 17 â€” Issue Signed Civic Documents (Week 17)

| Day | Quest | Key Constraint | Ship |
|-----|-------|----------------|------|
| Mon | Document Schema | Schema includes issuer ID, issue time, expiration, canonical hash | `week-17/day1-document-schema.md` |
| Tue | Issuer Key Policy | Verification supports prior active keys within overlap window | `week-17/day2-issuer-key-policy.md` |
| Wed | Issue Workflow | Issued document content is immutable after signature | `week-17/day3-issue-workflow.md` |
| Thu | Verify & Revocation Rules | Revoked issuer/doc fails verification even with valid signature | `week-17/day4-verify-revocation-rules.md` |
| Fri | Policy Gates | Policy validation occurs before signing | `week-17/day5-policy-gates.md` |
| **Sat** ğŸ—¡ï¸ | **BOSS: Issuance Demo** | Full traceability from request ID to signed document hash | `week-17/day6-issuance-demo.md` |

---

## ğŸ““ Chapter 18 â€” Transparency Log Anchoring (Week 18)

| Day | Quest | Key Constraint | Ship |
|-----|-------|----------------|------|
| Mon | Anchoring Workflow | Issuance not "final" until anchoring receipt obtained | `week-18/day1-anchoring-workflow.md` |
| Tue | Receipt Bundle Schema | Bundle must be self-contained for offline verification | `week-18/day2-receipt-bundle-schema.md` |
| Wed | Anchor Verifier Sequence | Any missing step yields `unverified`, never soft-pass | `week-18/day3-anchor-verifier-sequence.md` |
| Thu | Receipt Freshness Policy | Reject receipts older than freshness window | `week-18/day4-receipt-freshness-policy.md` |
| Fri | Anchoring Attack Matrix | System distinguishes `not_anchored`, `tampered`, `stale` | `week-18/day5-anchoring-attack-matrix.md` |
| **Sat** ğŸ—¡ï¸ | **BOSS: Pipeline Integration** | Failed anchoring triggers compensating status, not silent success | `week-18/day6-pipeline-integration.md` |

---

## ğŸ““ Chapter 19 â€” Offline Verification Package (Week 19)

| Day | Quest | Key Constraint | Ship |
|-----|-------|----------------|------|
| Mon | Verifier UX Contract | One-line machine status + human explanation block | `week-19/day1-verifier-ux-contract.md` |
| Tue | Offline Bundle Format | Bundle verifies completeness before cryptographic checks | `week-19/day2-offline-bundle-format.md` |
| Wed | Air-Gap Verification Flow | No remote calls allowed in offline mode | `week-19/day3-airgap-verification-flow.md` |
| Thu | Time Policy Modes | Verifier output includes policy mode used (`strict`/`grace`/`archival`) | `week-19/day4-time-policy-modes.md` |
| Fri | Batch Verifier Rules | Per-document verdicts even if one bundle fails | `week-19/day5-batch-verifier-rules.md` |
| **Sat** ğŸ—¡ï¸ | **BOSS: Offline Verifier Guide** | Guide includes â‰¥5 common failure interpretations | `week-19/day6-offline-verifier-guide.md` |

---

## ğŸ““ Chapter 20 â€” Failure Survival Hardening (Week 20)

| Day | Quest | Key Constraint | Ship |
|-----|-------|----------------|------|
| Mon | Chaos Matrix | Each test specifies expected degraded mode + recovery trigger | `week-20/day1-chaos-matrix.md` |
| Tue | Node Crash Drill | No duplicate document issuance after leader crash | `week-20/day2-node-crash-drill.md` |
| Wed | Partition Drill | Minority partition cannot produce final anchored documents | `week-20/day3-partition-drill.md` |
| Thu | Key Compromise Runbook | Compromised key cannot sign new checkpoints after cutoff | `week-20/day4-key-compromise-runbook.md` |
| Fri | Restore Validation | Restored system proves continuity from last valid checkpoint | `week-20/day5-restore-validation.md` |
| **Sat** ğŸ—¡ï¸ | **ARC BOSS: Hardening Report** ğŸ† | Each unresolved risk has mitigation owner + timeline | `month-5-demo/hardening-report.md` + `week-20/day6-month5-report.md` |

> ### ğŸ† Achievement Unlocked: **Chaos Survivor**
> *You composed a full civic trust system and proved it survives crashes, partitions, and key compromise.*

---

# ğŸš€ Arc 6 â€” Ship It (Month 6)

> **ğŸ¯ Mission:** Package technical depth into employable proof.
>
> **ğŸ§  Mindset unlock:** *"Great engineering includes clear evidence, clear stories, and clear tradeoffs."*

```
Month 6 Progress
[â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0% â€” Week 21 Â· 22 Â· 23 Â· 24
```

---

## ğŸ“’ Chapter 21 â€” Reliability / SLO Story (Week 21)

| Day | Quest | Key Constraint | Ship |
|-----|-------|----------------|------|
| Mon | SLI/SLO Table | Each SLO maps to one user-visible outcome | `week-21/day1-sli-slo-table.md` |
| Tue | Metrics Design | Avoid high-cardinality labels that break observability | `week-21/day2-metrics-design.md` |
| Wed | Dashboard Spec | Dashboard shows SLO status + recent error-budget burn | `week-21/day3-dashboard-spec.md` |
| Thu | Alert Rules | Paging only for user-impacting or trust-critical conditions | `week-21/day4-alert-rules.md` |
| Fri | Capacity Plan | Include 2x surge headroom target | `week-21/day5-capacity-plan.md` |
| **Sat** ğŸ—¡ï¸ | **BOSS: Reliability Story** | Every guarantee statement references concrete evidence | `week-21/day6-reliability-story.md` |

---

## ğŸ“’ Chapter 22 â€” Security / Threat Model Story (Week 22)

| Day | Quest | Key Constraint | Ship |
|-----|-------|----------------|------|
| Mon | Threat Model Map (STRIDE) | Every component has explicit trust boundary + threat owner | `week-22/day1-threat-model-map.md` |
| Tue | Abuse Cases (Top 10) | Each includes detection signal + response action | `week-22/day2-abuse-cases.md` |
| Wed | Threat-Control Matrix | Every high-risk threat: â‰¥1 preventive + â‰¥1 detective control | `week-22/day3-threat-control-matrix.md` |
| Thu | Supply Chain & Secrets | No hardcoded secrets + documented rotation cadence | `week-22/day4-supplychain-secrets-policy.md` |
| Fri | Security Test Plan | Include one cross-component attack path test | `week-22/day5-security-test-plan.md` |
| **Sat** ğŸ—¡ï¸ | **BOSS: Security Story** | Explicitly list "not solved yet" risks | `week-22/day6-security-story.md` |

---

## ğŸ“’ Chapter 23 â€” Docs, Demos & Interview Narratives (Week 23)

| Day | Quest | Key Constraint | Ship |
|-----|-------|----------------|------|
| Mon | Architecture Diagram Plan | Every trust guarantee maps to one diagram element | `week-23/day1-architecture-diagram-plan.md` |
| Tue | README Outline | Quickstart reaches 1 successful verify in <15 min | `week-23/day2-readme-outline.md` |
| Wed | Demo Script | Include one planned failure + recovery segment | `week-23/day3-demo-script.md` |
| Thu | Video Storyboard | Each claim shown with on-screen evidence | `week-23/day4-video-storyboard.md` |
| Fri | Interview Story Bank (STAR) | 8 stories, each with one metric + one tradeoff | `week-23/day5-interview-story-bank.md` |
| **Sat** ğŸ—¡ï¸ | **BOSS: Portfolio Index** | Every major claim links to one artifact | `week-23/day6-portfolio-index.md` |

---

## ğŸ“’ Chapter 24 â€” Final Interview Prep & Publication (Week 24)

| Day | Quest | Key Constraint | Ship |
|-----|-------|----------------|------|
| Mon | Distributed Systems Q&A | Each answer includes one concrete project example | `week-24/day1-dist-sys-qa.md` |
| Tue | Trust Architecture Q&A | Include one "limitation" answer per security claim | `week-24/day2-trust-qa.md` |
| Wed | Debug Drills | Each drill ends with measurable confirmation step | `week-24/day3-debug-drills.md` |
| Thu | System Design Walkthrough | Separate MVP and hardening phases | `week-24/day4-system-design-walkthrough.md` |
| Fri | Final Demo Rehearsal | Complete in target time with one intentional failure drill | `week-24/day5-final-demo-scorecard.md` |
| **Sat** ğŸ—¡ï¸ | **FINAL BOSS: Publish & Retrospective** ğŸ† | Retrospective includes measurable before/after capability table | `month-6-final/README.md` + `month-6-final/retrospective.md` + `month-6-final/demo-script.md` |

> ### ğŸ† Achievement Unlocked: **Distributed Trust Engineer**
> *You shipped a civic-grade trust system with verified failure behavior, documented guarantees, and interview-ready proof.*

---

# ğŸš© Weekly Boss Fights (Checkpoints)

> **Rule:** Do NOT advance to the next chapter unless you pass the gate. No exceptions.

| Week | Demo Must Show | ğŸ’¥ Failure Drill | âœ… Pass If... |
|------|---------------|-----------------|--------------|
| **1** | CLI commands + logger + error catalog | Permission denied on log path | All exit codes match contract |
| **2** | Echo works + framing + timeout logs | Client disconnect mid-frame | No crash + clear error classification |
| **3** | 50+ clients + bounded buffers + churn | Slow reader with growing buffer | No fd leaks + stable under churn |
| **4** | Epoll + HTTP parser + e2e trace IDs | Malformed HTTP response header | Parser rejects safely + logs reason |
| **5** | Bounded queue + shutdown + metrics | Queue saturation | Deterministic overload + no task loss |
| **6** | Overload thresholds + slow-client defense | Slowloris-like sender | Predictable degradation, no memory runaway |
| **7** | Hash tool + protocol digest + corruption drill | One-byte payload tamper | Tamper always detected and rejected |
| **8** | Key policy + signed envelope + replay rules | Replay previously valid request | Replayed request rejected with reason |
| **9** | KV contract + versioning + signed commands | Duplicate request ID on mutation | Duplicate does not reapply |
| **10** | WAL schema + crash drill matrix | Crash after WAL append | Committed write survives, corrupt tail handled |
| **11** | Append RPC + quorum + partition policy | Follower lag then rejoin | Nodes converge to identical state hash |
| **12** | Election + retry/idempotency + fencing | Leader fails during client write | No lost commits + no duplicate effects |
| **13** | CAS model + chunks + GC + audit | Interrupted CAS write | Incomplete object never appears valid |
| **14** | Merkle rules + proofs + verifier + perf | Malformed inclusion proof | Verifier fails closed on all bad inputs |
| **15** | Log contract + inclusion/consistency + checkpoints | Inconsistent checkpoint continuity | Discontinuous chain rejected |
| **16** | Monitor + gossip + equivocation + runbook | Conflicting checkpoints same size | Conflict â†’ signed incident + critical alert |
| **17** | Doc schema + issuer + verify + revocation | Policy-violating issuance attempt | Policy violation blocks signing + logs |
| **18** | Anchoring + receipt bundle + freshness | Issue succeeds but anchoring fails | Document stays non-final, status explicit |
| **19** | Offline bundle + air-gap verifier + batch | Bundle missing one proof artifact | Deterministic `unverified` reason returned |
| **20** | Chaos matrix + crash/partition/key drills | Combined partition + leader crash | No duplicate issuance + recovery documented |
| **21** | SLI/SLO + metrics + dashboard + alerts | Synthetic SLO breach | Breach detected, alerted, explained |
| **22** | Threat map + abuse cases + controls | Replay + stale checkpoint attack | Detection + response steps documented |
| **23** | Diagram + README + demo + stories + index | Teammate follows quickstart with mistake | User recovers via docs without help |
| **24** | Q&A + drills + design + demo scorecard | Live demo component outage | Fallback preserves trust narrative in time |

### ğŸ”„ Weekly Reflection Prompts

After each weekly gate, answer these three questions in your notes:
1. **What surprised you?** â€” the unexpected behavior or bug
2. **What's the weakest link?** â€” the component you trust least
3. **What carries forward?** â€” what artifact or insight feeds next week

---

# ğŸ“Š Monthly Level-Ups

### ğŸŒ Month 1 â€” Level Up: **Byte Wrangler**
| | |
|---|---|
| **Can build now** | Event-loop TCP service with framing/timeouts, reusable CLI/logger, HTTP client |
| **Biggest risk** | Uncontrolled load + no crypto trust |
| **Next month fixes** | Thread pool, backpressure, hashes, signatures, replay defense |
| **ğŸ“¦ Publish** | `month-1-demo/README.md` + architecture diagram + latency table |

### âš”ï¸ Month 2 â€” Level Up: **Trust Forger**
| | |
|---|---|
| **Can build now** | Signed, replay-protected network protocol with overload controls |
| **Biggest risk** | Single-node state loss + no replication |
| **Next month fixes** | WAL recovery, quorum replication, election, idempotent retries |
| **ğŸ“¦ Publish** | `month-2-demo/README.md` + signed envelope diagram + tamper demo |

### ğŸ° Month 3 â€” Level Up: **Fault Tamer**
| | |
|---|---|
| **Can build now** | Durable replicated KV cluster surviving crashes/failovers with safe retries |
| **Biggest risk** | No external transparency evidence |
| **Next month fixes** | CAS, Merkle proofs, transparency log, monitors |
| **ğŸ“¦ Publish** | `month-3-demo/README.md` + replication diagram + failover demo |

### ğŸ”® Month 4 â€” Level Up: **Trust Architect**
| | |
|---|---|
| **Can build now** | Tamper-evident transparency subsystem with proofs + signed checkpoints + monitoring |
| **Biggest risk** | Missing product workflow integration |
| **Next month fixes** | Issuance, anchoring, offline verification, failure survival |
| **ğŸ“¦ Publish** | `month-4-demo/README.md` + trust architecture diagram + equivocation demo |

### ğŸ‘‘ Month 5 â€” Level Up: **Chaos Survivor**
| | |
|---|---|
| **Can build now** | CivicTrust system: signed docs, transparency anchoring, offline verify, failure survival |
| **Biggest risk** | Weak employability packaging |
| **Next month fixes** | SLO narrative, threat model, polished docs/demos/interview prep |
| **ğŸ“¦ Publish** | `month-5-demo/README.md` + end-to-end dataflow diagram + failure drill reel |

### ğŸš€ Month 6 â€” Level Up: **Distributed Trust Engineer**
| | |
|---|---|
| **Can build now** | End-to-end distributed trust platform with verified guarantees + interview narratives |
| **Next challenge** | Byzantine tolerance, formal verification, or production deployment |
| **ğŸ“¦ Publish** | `month-6-final/README.md` + final diagram + demo video + Q&A pack |

---

## ğŸ“ You Beat the Game. Now What?

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                      â•‘
â•‘   6 months ago: "I want to learn distributed systems"                â•‘
â•‘                                                                      â•‘
â•‘   Now: "I can show running systems, failure behavior,                â•‘
â•‘         and verification evidence."                                  â•‘
â•‘                                                                      â•‘
â•‘   Your proof is not a certificate. It's a portfolio of               â•‘
â•‘   systems that break on purpose and recover on command.              â•‘
â•‘                                                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Next 90-day paths:**
- ğŸ”´ **Hard mode:** Add Byzantine fault tolerance
- ğŸŸ¡ **Research mode:** Formal verification with TLA+
- ğŸŸ¢ **Ship mode:** Deploy to real infrastructure

---

> *Original backup: `Distributed_Trust_Engineer_24_Week_Roadmap_BACKUP.md`*
